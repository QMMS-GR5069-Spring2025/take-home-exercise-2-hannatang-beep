{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01255ae9-9f90-443b-8540-16bdb3e1ad3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col, round, avg, sum, count, desc, row_number\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import floor, months_between, to_date, min, max, first\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed76b732-efdb-4b8d-aaf3-347feeb6abc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_laptimes = spark.read.csv('s3://columbia-gr5069-main/raw/lap_times.csv', header=True)\n",
    "display(df_laptimes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13ceea50-dc42-4151-ac95-6dbccb988841",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_drivers = spark.read.csv('s3://columbia-gr5069-main/raw/drivers.csv', header=True)\n",
    "display(df_drivers)\n",
    "df_drivers.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e23c244c-521b-44a3-982b-9fbf3a4408b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pitstops = spark.read.csv('s3://columbia-gr5069-main/raw/pit_stops.csv', header=True)\n",
    "display(df_pitstops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83bb9f38-3660-49a9-b3db-511d386a029e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###1. Average time at the pits \n",
    "provide the average time that each driver spent at the pit stop for each race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14b4f2a2-0313-4911-8afe-9796fef8ed85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#avg pit stop time\n",
    "df_avg_pit_time = (\n",
    "    df_pitstops\n",
    "    .groupBy(\"raceId\", \"driverId\")\n",
    "    .agg(avg(\"duration\").alias(\"avg_pit_time\"))\n",
    "    .orderBy(\"raceId\", \"avg_pit_time\")  \n",
    ")\n",
    "df_avg_pit_time.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1deba2a2-051a-4a0c-a049-31806310877e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "weird... why raceID start with 1000 instead of 841?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "191334db-881f-4f73-b7d2-edb4527ee9e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Checking data type of pitstops\n",
    "df_pitstops.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0b555b8-5614-4f74-8563-b90edf0dd9d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Change data type of raceId from String to Int.\n",
    "df_pitstops = df_pitstops.withColumn(\"raceId\", col(\"raceId\").cast(\"int\"))\n",
    "\n",
    "# run again\n",
    "# avg pit stop time, round to 3th decimal place\n",
    "df_avg_pit_time = (\n",
    "    df_pitstops\n",
    "    .groupBy(\"raceId\", \"driverId\")\n",
    "    .agg(round(avg(\"duration\"), 3).alias(\"avg_pit_time\"))\n",
    "    .orderBy(\"raceId\", \"avg_pit_time\")  \n",
    ")\n",
    "df_avg_pit_time.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fef4f75c-3b29-4a80-8d49-e1e7f8c3d0e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###2. Rank average pit stop\n",
    " rank the average time spent at the pit stop (from previous question) and use the finishing order on the race to rank order these average times. Add in your comments how you decided to deal with drivers who did not finish the race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bed25d8-61ec-4dae-8208-c488059f49f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# finishing order\n",
    "df_results = spark.read.csv('s3://columbia-gr5069-main/raw/results.csv', header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ceeb0067-d976-4bf8-9e23-8d13489ffad1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# filter out the raceId, driverId, positionOrder, statusId columns\n",
    "df_results_filtered = df_results.select(\"raceId\", \"driverId\", \"positionOrder\")\n",
    "\n",
    "#join with avg pit stop time\n",
    "df_combined = df_avg_pit_time.join(df_results_filtered, on=[\"raceId\", \"driverId\"], how=\"left\")\n",
    "df_combined.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5df12ae6-3c26-4fac-a63e-59929854c9e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# place order in each race\n",
    "df_combined = df_combined.orderBy(\"raceId\", \"positionOrder\")\n",
    "df_combined.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d2725fc-bfdb-428a-8d4e-3f5a4890b9b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# change data type of posistionOrder from String to Int\n",
    "df_combined = df_combined.withColumn(\"positionOrder\", col(\"positionOrder\").cast(\"int\"))\n",
    "\n",
    "# run again\n",
    "df_combined = df_combined.orderBy(\"raceId\", \"positionOrder\")\n",
    "df_combined.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10e606e1-628f-4db8-8cbc-089cca1e221b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "I ranked the average pit stop time for each race by ordering the drivers based on their finishing places. This allows us to observe pit stop efficiency in the context of the race outcome.\n",
    "For drivers who did not finish the race (DNF), I kept them in the dataset but their finishing position might be less meaningful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c61ba66f-03d7-454e-939c-bf18725397dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3. insert the missing code\n",
    "insert a three letter code where codes are missing in the driver dataset. Write in comments how you arrived at these codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e606dc93-9236-4846-9dd6-336222a8e336",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# check the column of driver dataset\n",
    "df_drivers.printSchema()\n",
    "df_drivers.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3b52ee8-c0b0-4944-87e2-52cace024006",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "I checked for the schema that the column \"code\" is a string and it is nullable, so I will check any missing 3-letter driver codes in the dataset using a null filter on the column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ee7acfa-2f37-4b40-9ac1-a016e23314da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# filter missing code\n",
    "df_missing_code = df_drivers.filter(col(\"code\").isNull())\n",
    "df_missing_code.select(\"driverId\", \"forename\", \"surname\").show()\n",
    "df_drivers.filter(col(\"code\").isNull()).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e7a9a14-55d6-4423-9e13-e2c649a35c18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The result returned no missing values, which means all drivers already have valid codes. Therefore... no manual insert is needed at this stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b00bcf3e-1ad6-479e-a10e-df0b7df7d795",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4. youngest and oldest driver\n",
    "- create a new column called “Age” that counts how many birthdays each driver has had in their lives\n",
    "- explain how you reached this number in your comments\n",
    "- identify the oldest and youngest driver for each race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2acbfc28-05b1-4fef-998b-9e247be02881",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# get the race data\n",
    "df_races = spark.read.csv('s3://columbia-gr5069-main/raw/races.csv', header=True)\n",
    "df_races = df_races.select(\"raceId\", \"date\")\n",
    "\n",
    "# get driver date of birth\n",
    "df_driver_dob = df_drivers.select(\"driverId\", \"dob\")\n",
    "\n",
    "# join race date, dob \n",
    "df_driver_age = (\n",
    "    df_results\n",
    "    .select(\"raceId\", \"driverId\")\n",
    "    .join(df_driver_dob, on=\"driverId\", how=\"left\")\n",
    "    .join(df_races, on=\"raceId\", how=\"left\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc31fc83-ae72-475f-8c60-4c356ab906ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ensure formate to date\n",
    "df_driver_age = df_driver_age.withColumn(\"race_date\", to_date(\"date\"))\n",
    "df_driver_age = df_driver_age.withColumn(\"dob\", to_date(\"dob\"))\n",
    "\n",
    "# calculate age: total months(race date - dob) / 12 = age\n",
    "df_driver_age = df_driver_age.withColumn(\n",
    "    \"Age\",\n",
    "    floor(months_between(col(\"race_date\"), col(\"dob\")) / 12)\n",
    ")\n",
    "\n",
    "# find the youngest driver for each race\n",
    "window_young = Window.partitionBy(\"raceId\").orderBy(\"Age\")\n",
    "df_youngest = df_driver_age.withColumn(\"rank_young\", rank().over(window_young)) \\\n",
    "                           .filter(col(\"rank_young\") == 1) \\\n",
    "                           .select(\"raceId\", col(\"driverId\").alias(\"youngest_driverId\"), col(\"Age\").alias(\"youngest_age\"))\n",
    "\n",
    "# find the oldest driver for each race\n",
    "window_old = Window.partitionBy(\"raceId\").orderBy(col(\"Age\").desc())\n",
    "df_oldest = df_driver_age.withColumn(\"rank_old\", rank().over(window_old)) \\\n",
    "                         .filter(col(\"rank_old\") == 1) \\\n",
    "                         .select(\"raceId\", col(\"driverId\").alias(\"oldest_driverId\"), col(\"Age\").alias(\"oldest_age\"))\n",
    "\n",
    "\n",
    "df_extremes = df_youngest.join(df_oldest, on=\"raceId\", how=\"inner\").orderBy(\"raceId\")\n",
    "#change data type of raceId from String to Int\n",
    "df_extremes = df_extremes.withColumn(\"raceId\", col(\"raceId\").cast(\"int\")).orderBy(\"raceId\")\n",
    "df_extremes.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "763bec83-7459-4086-a490-e7016a7a09b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "I computed each driver's age at the time of the race by using their date of birth and the race date.\n",
    "I used the formula `months_between(race_date, dob) / 12` and applied floor() to count only full birthdays (integer age).\n",
    "Then I grouped the data by raceId and used min() and max() to find the youngest and oldest drivers per race.\n",
    "In some races, multiple drivers shared the exact same age, resulting in ties for youngest or oldest. In such cases, I retained all tied drivers to fully reflect the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "234bc73d-b8e2-4e02-9b36-91d1695c2d8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 5.  most wins and loses per race\n",
    "- on a given race, provide a count of how many times a diver has won previous races, and a count of all the times a driver has not won (but completed) a race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "207ae999-059d-4e0a-bad1-9cd6fd290072",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Check statusID meaning\n",
    "df_results_raw = spark.read.csv(\"s3://columbia-gr5069-main/raw/results.csv\", header=True)\n",
    "df_status = spark.read.csv('s3://columbia-gr5069-main/raw/status.csv', header=True)\n",
    "df_status.show()\n",
    "df_status = df_status.withColumnRenamed(\"status\", \"statusText\")\n",
    "\n",
    "df_results_1 = df_results_raw.join(df_status, on=\"statusId\", how=\"left\")\n",
    "df_results_1.select(\"raceId\", \"driverId\", \"statusId\", \"statusText\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "941aaa61-7072-4b98-8b7e-511e70c2edcd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# add is_win, is_completed, is_completed_not_win columns\n",
    "df_results_2 = df_results_1 \\\n",
    "    .withColumn(\"is_win\", when(col(\"positionOrder\") == 1, 1).otherwise(0)) \\\n",
    "    .withColumn(\"is_completed\", when(\n",
    "        (col(\"statusText\").contains(\"Finished\")) | (col(\"statusText\").rlike(\"\\\\+[0-9]+ Laps?\")),\n",
    "        1\n",
    "    ).otherwise(0)) \\\n",
    "    .withColumn(\"is_completed_not_win\", when(\n",
    "        (col(\"is_completed\") == 1) & (col(\"positionOrder\") != 1), 1\n",
    "    ).otherwise(0))\n",
    "\n",
    "\n",
    "df_results_2.select(\"raceId\", \"driverId\", \"positionOrder\", \"statusText\", \"is_win\", \"is_completed\", \"is_completed_not_win\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e138f18e-1193-427d-8fc9-913821571a6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "now we transfer the race result status to binary data and we can calculate how many times a diver has won previous races."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4755ff3d-f378-4d0e-934e-a75bfa11dfce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#import race data\n",
    "df_races = spark.read.csv('s3://columbia-gr5069-main/raw/races.csv', header=True, inferSchema=True) \\\n",
    "    .select(\"raceId\", \"date\") \\\n",
    "    .withColumn(\"race_date\", to_date(col(\"date\")))\n",
    "\n",
    "\n",
    "# join race date with results\n",
    "df_results_with_date = df_results_2.join(df_races.select(\"raceId\", \"race_date\"), on=\"raceId\", how=\"left\")\n",
    "\n",
    "#partition by driver ID\n",
    "window_spec = Window.partitionBy(\"driverId\").orderBy(\"race_date\").rowsBetween(Window.unboundedPreceding, -1)\n",
    "\n",
    "# add stats data\n",
    "df_driver_history = df_results_with_date \\\n",
    "    .withColumn(\"total_wins_before\", sum(\"is_win\").over(window_spec)) \\\n",
    "    .withColumn(\"total_completes_not_win_before\", sum(\"is_completed_not_win\").over(window_spec))\n",
    "\n",
    "\n",
    "df_driver_history.select(\n",
    "    \"raceId\", \"driverId\", \"race_date\", \"positionOrder\",\n",
    "    \"is_win\", \"is_completed_not_win\",\n",
    "    \"total_wins_before\", \"total_completes_not_win_before\"\n",
    ").orderBy(\"raceId\", \"driverId\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d536f59f-6c89-4e07-a24a-5a2dc2afd32d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "For each driver and each race, I calculated the cumulative number of wins and completed but not won races prior to the current race.\n",
    "I used a window function over the driver’s race history ordered by race date, and aggregated:\n",
    "`is_win to get total_wins_before\n",
    "is_completed_not_win to get total_completes_not_win_before`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7f04e03-a64b-4fcc-ac0b-c38c021e0415",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 6. own question - TOP Crusher 💸\n",
    "Find the most costly drivers per season — the ones who crashed the most and caused the highest repair costs for their teams. 💸💸💸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ebe2a5a-c95f-4957-b4d0-2db13a406dac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Join status, driver, race, results data\n",
    "df_status = spark.read.csv(\"s3://columbia-gr5069-main/raw/status.csv\", header=True).withColumnRenamed(\"status\", \"statusText\")\n",
    "df_drivers = spark.read.csv(\"s3://columbia-gr5069-main/raw/drivers.csv\", header=True, inferSchema=True) \\\n",
    "    .select(\"driverId\", \"forename\", \"surname\")\n",
    "df_races = spark.read.csv(\"s3://columbia-gr5069-main/raw/races.csv\", header=True, inferSchema=True).select(\"raceId\", \"year\")\n",
    "df_results = spark.read.csv(\"s3://columbia-gr5069-main/raw/results.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Step 2: Join status + race year\n",
    "df_incidents = df_results.join(df_status, on=\"statusId\", how=\"left\") \\\n",
    "                         .join(df_races, on=\"raceId\", how=\"left\")\n",
    "\n",
    "# Step 3: Define which statusText are considered crash-related incidents that likely incurred repair costs\n",
    "crash_keywords = [\"Collision\", \"Accident\", \"Spun off\", \"Damage\"]\n",
    "\n",
    "df_incidents = df_incidents.withColumn(\n",
    "    \"is_crash\",\n",
    "    when(col(\"statusText\").isin(crash_keywords), 1).otherwise(0)\n",
    ")\n",
    "\n",
    "# Step 4: each season x driver crushed time\n",
    "df_crash_counts = df_incidents.groupBy(\"year\", \"driverId\") \\\n",
    "    .agg(count(when(col(\"is_crash\") == 1, True)).alias(\"crash_count\"))\n",
    "\n",
    "# Step 5: find out the most crashed driver in each season\n",
    "window_spec = Window.partitionBy(\"year\").orderBy(desc(\"crash_count\"))\n",
    "\n",
    "df_top_crashers = df_crash_counts.withColumn(\"rank\", row_number().over(window_spec)) \\\n",
    "    .filter(col(\"rank\") == 1) \\\n",
    "    .orderBy(\"year\")\n",
    "\n",
    "# join driver name\n",
    "df_top_crashers_named = df_top_crashers.join(df_drivers, on=\"driverId\", how=\"left\")\n",
    "\n",
    "#Show results\n",
    "df_top_crashers_named.select(\"year\", \"driverId\", \"forename\", \"surname\", \"crash_count\").orderBy(\"year\").show()               "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "etl_pipeline_f1_Take Home Exercise #2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
